{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7896209,"sourceType":"datasetVersion","datasetId":4636754}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport json\n\n# Load training data\ndef load_data(file_path):\n    with open(file_path, 'r') as f:\n        data = [json.loads(line) for line in f]\n    return data\n\ntrain_data = load_data('/kaggle/input/conll2003-posdataset/train.jsonl')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-31T02:56:40.563258Z","iopub.execute_input":"2024-03-31T02:56:40.563935Z","iopub.status.idle":"2024-03-31T02:56:40.786299Z","shell.execute_reply.started":"2024-03-31T02:56:40.563894Z","shell.execute_reply":"2024-03-31T02:56:40.785338Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"inputs = []\ntargets = []\nfor sentence in train_data:\n    tokens = sentence['tokens']\n    chunk_tags = sentence['chunk_tags']\n    pos_tags = sentence['pos_tags']\n    inputs.append(pos_tags)\n    targets.append(chunk_tags)\n#     print(pos_tags)\n# Calculate the index to split the list\nsplit_index = int(0.8 * len(original_list))\n\n# Split the list\ntrain_set = original_list[:split_index]\ntest_set = original_list[split_index:]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T23:50:46.514240Z","iopub.execute_input":"2024-03-30T23:50:46.514636Z","iopub.status.idle":"2024-03-30T23:50:46.530287Z","shell.execute_reply.started":"2024-03-30T23:50:46.514605Z","shell.execute_reply":"2024-03-30T23:50:46.529361Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define sigmoid activation function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Initialize weights and biases\ninput_size = 4  # Number of POS tags\nhidden_size = 9\noutput_size = 1\nlearning_rate = 0.001\n\nWxh = np.random.randn(hidden_size, input_size) * 0.01\nWhh = np.random.randn(hidden_size, hidden_size) * 0.01\nWhy = np.random.randn(output_size, hidden_size) * 0.01\nbh = np.zeros((hidden_size, 1))\nby = np.zeros((output_size, 1))\n\n# Forward propagation\ndef forward_propagation(inputs):\n    h = np.zeros((hidden_size, 1))\n    for i in range(len(inputs)):\n        x = np.zeros((input_size, 1))\n        x[inputs[i] - 1] = 1  # One-hot encoding of POS tag\n        h = np.dot(Wxh, x) + np.dot(Whh, h) \n    output = sigmoid(np.dot(Why, h))\n    return output, h\n\n# Backpropagation through time\ndef backward_propagation(inputs, targets, hprev):\n    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n    dhnext = np.zeros_like(hprev)\n    for i in reversed(range(len(inputs))):\n        x = np.zeros((input_size, 1))\n        x[inputs[i] - 1] = 1  # One-hot encoding of POS tag\n        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hprev) + bh)\n        output = sigmoid(np.dot(Why, h) + by)\n        error = output - targets[i]\n        dWhy += np.dot(error, h.T)\n        dby += error\n        dh = np.dot(Why.T, error) + dhnext\n        dhraw = (1 - h * h) * dh\n        dbh += dhraw\n        dWxh += np.dot(dhraw, x.T)\n        dWhh += np.dot(dhraw, hprev.T)\n        dhnext = np.dot(Whh.T, dhraw)\n    return dWxh, dWhh, dWhy, dbh, dby\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:47.085179Z","iopub.execute_input":"2024-03-30T20:45:47.085452Z","iopub.status.idle":"2024-03-30T20:45:47.099809Z","shell.execute_reply.started":"2024-03-30T20:45:47.085393Z","shell.execute_reply":"2024-03-30T20:45:47.098865Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# # Training loop\n# grad = []\n# for epoch in range(1):\n#     total_loss = 0\n#     total_words = 0\n    \n#     for sentence in train_data:\n#         tokens = sentence['tokens']\n#         pos_tags = sentence['pos_tags']\n#         chunk_tags = sentence['chunk_tags']\n        \n#         hidden_state = np.zeros((hidden_size, 1))  # Initialize hidden state\n        \n#         for i in range(len(tokens)):\n#             # Prepare input and target\n#             input_pos = pos_tags[i]\n#             target_chunk = chunk_tags[i]\n            \n#             # Forward propagation\n#             output, hidden_state = forward_propagation([input_pos])\n#             prediction = output[0][0]\n            \n#             # Calculate loss\n#             loss = (prediction - target_chunk) ** 2\n#             total_loss += loss\n#             total_words += 1\n            \n#             # Backpropagation through time\n#             dWxh, dWhh, dWhy, dbh, dby = backward_propagation([input_pos], [target_chunk], hidden_state)\n#             grad.append([dWxh, dWhh, dWhy, dbh, dby])\n#             print(dWxh)\n# #             print(\"hello\")\n#             # Update weights and biases\n#             Wxh -= learning_rate * dWxh\n#             Whh -= learning_rate * dWhh\n#             Why -= learning_rate * dWhy\n#             bh -= learning_rate * dbh\n#             by -= learning_rate * dby\n# #             break\n#         break\n#     break\n    \n#     # Calculate average loss\n#     average_loss = total_loss / total_words if total_words > 0 else 0\n    \n#     print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:47.101362Z","iopub.execute_input":"2024-03-30T20:45:47.101618Z","iopub.status.idle":"2024-03-30T20:45:47.113116Z","shell.execute_reply.started":"2024-03-30T20:45:47.101597Z","shell.execute_reply":"2024-03-30T20:45:47.112267Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Define sigmoid activation function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Initialize weights and biases\ninput_size = 4  # Number of POS tags\nhidden_size = 50\noutput_size = 1\nlearning_rate = 0.001\n\nWxh = np.random.randn(hidden_size, input_size) * 0.01\nWhh = np.random.randn(hidden_size, hidden_size) * 0.01\nWhy = np.random.randn(output_size, hidden_size) * 0.01\nbh = np.zeros((hidden_size, 1))\nby = np.zeros((output_size, 1))\n\n# Forward propagation\ndef forward_propagation(inputs):\n    h = np.zeros((hidden_size, 1))\n    for i in range(len(inputs)):\n        x = np.zeros((input_size, 1))\n        x[inputs[i] - 1] = 1  # One-hot encoding of POS tag\n        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n    output = sigmoid(np.dot(Why, h) + by)\n    return output, h\n\n# Backpropagation through time\ndef backward_propagation(inputs, targets, hprev=0):\n    curr_inputs = \n    prev_inputs = \n    output = forward(curr_inputs, prev_inputs, targets, hprev=0) # gives a list of 0 to 1 of size of each sentence\n    \n    \n    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n#     dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n    dhnext = np.zeros_like(hprev)\n    for i in reversed(range(len(inputs))):\n#         x = np.zeros((input_size, 1))\n#         x[inputs[i] - 1] = 1  # One-hot encoding of POS tag\n#         h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hprev) + bh)\n#         output = sigmoid(np.dot(Why, h) + by) # here output is single value, so put for loop here\n# #         error = output - targets[i]\n#         error = binary_cross_entropy(output,targets[i]) #output is a list of 0 and 1\n        dWhy += np.dot(error, h.T)\n        dby += error\n        dh = np.dot(Why.T, error) + dhnext\n#         dhraw = (1 - h * h) * dh\n#         dbh += dhraw\n        dWxh_curr += np.dot(dh, x_curr.T) #dhraw\n        dWxh_prev += np.dot(dh, x_prev.T) #dhraw\n#         dWhh += np.dot(dh, hprev.T) #dhraw\n        dhnext = np.dot(Why.T, dh) #dhraw , Whx\n    return dWxh, dWhh, dWhy, dbh, dby\n\n# Binary cross-entropy loss\ndef binary_cross_entropy_loss(prediction, target):\n    epsilon = 1e-15  # Small value to prevent NaN in log\n    return -(target * np.log(prediction + epsilon) + (1 - target) * np.log(1 - prediction + epsilon))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:47.174675Z","iopub.execute_input":"2024-03-30T20:45:47.175405Z","iopub.status.idle":"2024-03-30T20:45:47.188644Z","shell.execute_reply.started":"2024-03-30T20:45:47.175376Z","shell.execute_reply":"2024-03-30T20:45:47.187396Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[30], line 31\u001b[0;36m\u001b[0m\n\u001b[0;31m    curr_inputs =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (1968161033.py, line 31)","output_type":"error"}]},{"cell_type":"code","source":"def one_hot(pos_tags): #returns an array of one hot encoding of each words in a sentence\n    X_new = np.zeros((1, len(pos_tags), 4))  # 4 is the number of POS tag categories\n    for i, tag in enumerate(pos_tags):\n        X_new[0, i, tag-1] = 1\n    # print(X_new)\n    arrays = [np.reshape(array, (4, 1)) for array in X_new[0]] #One hot encoded arrays\n    return arrays\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef binary_cross_entropy(y_pred, y_true):\n    epsilon = 1e-15  # to prevent log(0) case\n    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # clipping to avoid numerical instability\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef bce_derivative(y_pred, y_true):\n    epsilon = 1e-15  # to prevent division by zero\n    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # clipping to avoid numerical instability\n    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n\ndef forward(word, prev_inputs, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh): #Woh_hh = 1 initially\n#     for word in sent:\n    h_t = np.dot(Wxh_curr, word) + np.dot(Wxh_prev, prev_inputs) + np.dot(Woh_hh, prev_hid_output)\n    output = sigmoid(np.dot(Woh_hh, h_t))\n    prev_inputs = np.insert(word, 0, 0, axis=0)\n    prev_inputs = np.array([prev_input])\n    return prev_inputs, h_t, output # return output as a single integer\n\ndef bptt(sent, prev_inputs, prev_hid_output, target, Wxh_curr, Wxh_prev, Woh_hh, total_loss, total_words):\n    prev_inp = []\n    h_t = []\n    output = []\n    for word in sent:\n        prev_inputs, h_t, output = forward(word, prev_inputs, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh)\n        prev_hid_output = output\n        prev_inp.append(prev_inputs) #this is an array containing prev_inputs of size 5x1\n        h_t.append(h_t)\n        output.append(output)\n        \n    dWxh_curr, dWxh_prev, dWoh_hh = np.zeros_like(Wxh_curr), np.zeros_like(Wxh_prev), np.zeros_like(Woh_hh)\n    dh_next = np.zeros_like(prev_hid_output)\n    for i in reversed(range(len(sent))):\n        loss = binary_cross_entropy(output[i], target[i])\n        total_loss += loss\n        total_words += 1\n        dWoh_hh += np.dot(bce_derivative(output[i], target[i]), h_t[i].T) \n        dh = np.dot(Woh_hh.T, bce_derivative(output[i], target[i])) + dh_next\n        dWhx_curr += np.dot(dh, sent[i].T)\n        dWhx_prev += np.dot(dh, prev_inp[i].T)\n        dh_next = np.dot(Woh_hh.T, dh)\n    return dWxh_curr, dWhx_prev, dWoh_hh, total_loss, total_words\n    \ndef train(inputs, prev_input,prev_hid_output, targets, Wxh_curr, Wxh_prev, Woh_hh, lr):\n    total_loss = 0\n    total_words = 0\n    for epoch in range(5):\n        print(\"Epoch:\", epoch)\n        for idx, sent in enumerate(inputs): # enumerate will return index value and value itself\n            dWxh_curr, dWxh_prev, dWoh_hh, total_loss, total_words = backpropagation(one_hot(sent), prev_inputs, prev_hid_output, targets[idx], Wxh_curr, Wxh_prev, Woh_hh, total_loss, total_words)\n            Wxh_curr -= lr * dWxh_curr\n            Wxh_prev -= lr * dWhx_prev\n            Woh_hh -= lr * dWoh_hh\n        average_loss = total_loss / total_words if total_words > 0 else 0\n        print(\"average_loss:\", average_loss)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:47.295392Z","iopub.execute_input":"2024-03-30T20:45:47.295772Z","iopub.status.idle":"2024-03-30T20:45:47.317019Z","shell.execute_reply.started":"2024-03-30T20:45:47.295743Z","shell.execute_reply":"2024-03-30T20:45:47.315842Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def one_hot(pos_tags): #returns an array of one hot encoding of each words in a sentence\n    X_new = np.zeros((1, len(pos_tags), 4))  # 4 is the number of POS tag categories\n    for i, tag in enumerate(pos_tags):\n        X_new[0, i, tag-1] = 1\n    arrays = [np.reshape(array, (4, 1)) for array in X_new[0]] #One hot encoded arrays\n    return arrays\n\ndef sigmoid(x):\n    if x >= 0:\n        return 1 / (1 + np.exp(-x))\n    else:\n        return np.exp(x) / (1 + np.exp(x))\n\ndef binary_cross_entropy(y_pred, y_true):\n    epsilon = 1e-15  # to prevent log(0) case\n    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # clipping to avoid numerical instability\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef bce_derivative(y_pred, y_true):\n    epsilon = 1e-15  # to prevent division by zero\n    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # clipping to avoid numerical instability\n    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n\ndef forward(word, prev_inputs, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh): #Woh_hh = 1 initially\n#     for word in sent:\n    h_t = np.dot(Wxh_curr, word) + np.dot(Wxh_prev, prev_inputs) + np.dot(Woh_hh, prev_hid_output)\n    output = sigmoid(h_t)\n    prev_inputs = np.insert(word, 0, 0, axis=0)\n    return prev_inputs, h_t, output # return output as a single integer\n\ndef bptt(sent, prev_inputs, prev_hid_output, target, Wxh_curr, Wxh_prev, Woh_hh, total_loss, total_words):\n    prev_inp = []\n    h_ts = []\n    outputs = []\n    for word in sent:\n        prev_input, h_t, output = forward(word, prev_inputs, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh)\n        prev_hid_output = output\n        prev_inputs = prev_input\n        prev_inp.append(prev_input)#this is an array containing output of all the words\n        h_ts.append(h_t)\n        outputs.append(output)\n        \n    dWxh_curr, dWxh_prev, dWoh_hh = np.zeros_like(Wxh_curr), np.zeros_like(Wxh_prev), np.zeros_like(Woh_hh)\n    dh_next = np.zeros_like(prev_hid_output)\n    for i in reversed(range(len(sent))):\n        loss = binary_cross_entropy(outputs[i], target[i])\n#         loss = (outputs[i], target[i])\n#         print(loss)\n#         print(total_loss)\n#         break\n        total_loss += loss\n        total_words += 1\n#         print(total_words)\n#         break\n        dWoh_hh += np.dot(bce_derivative(outputs[i], target[i]), sigmoid_derivative(h_ts[i]) * outputs[i].T) \n        dh = np.dot(Woh_hh.T, bce_derivative(outputs[i], target[i])) + dh_next\n        dWxh_curr += np.dot(dh, sent[i].T)\n        dWxh_prev += np.dot(dh, prev_inp[i].T)\n        dh_next = np.dot(Woh_hh.T, dh)\n    return dWxh_curr, dWxh_prev, dWoh_hh, loss, total_loss, total_words\n    \ndef train(inputs, prev_input, prev_hid_output, targets, Wxh_curr, Wxh_prev, Woh_hh, lr):\n    total_loss = 0\n    total_words = 0\n    for epoch in range(5):\n        print(\"Epoch:\", epoch)\n        for idx, sent in enumerate(inputs): # enumerate will return index value and value itself\n            dWxh_curr, dWxh_prev, dWoh_hh, loss, total_loss, total_words = bptt(one_hot(sent), prev_input, prev_hid_output, targets[idx], Wxh_curr, Wxh_prev, Woh_hh, total_loss, total_words)\n            Wxh_curr -= lr * dWxh_curr\n            Wxh_prev -= lr * dWxh_prev\n            Woh_hh -= lr * dWoh_hh\n        average_loss = total_loss / total_words if total_words > 0 else 0\n        print(\"loss:\", loss)\n        print(\"total_loss:\", total_loss)\n        print(\"total_words:\", total_words)\n        print(\"average_loss:\", average_loss)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T02:57:30.243280Z","iopub.execute_input":"2024-03-31T02:57:30.243903Z","iopub.status.idle":"2024-03-31T02:57:30.264285Z","shell.execute_reply.started":"2024-03-31T02:57:30.243871Z","shell.execute_reply":"2024-03-31T02:57:30.263239Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# inputs = [[1,4,3,1,4,4,3,1,4],[4,1,4,1,4,1,4,1,4]]\n# one_hot(pos_tags)[0] # 4x1\nprev_input = np.array([[1,0,0,0,0]]).T #shape 5x1\nprev_hid_output = np.array([[0]]).T #shape 1x1 \n# targets = [[1,1,1,0,1,1,1,0,1],[1,0,0,0,1,1,1,0,1]]\nWxh_curr = np.random.randn(1,4)\nWxh_prev = np.random.rand(1,5)\nWoh_hh = np.random.randn(1,1)\nlr = 0.01\n\ntrain(inputs, prev_input, prev_hid_output, targets, Wxh_curr, Wxh_prev, Woh_hh, lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T22:34:38.428929Z","iopub.execute_input":"2024-03-30T22:34:38.429299Z","iopub.status.idle":"2024-03-30T22:34:41.549387Z","shell.execute_reply.started":"2024-03-30T22:34:38.429269Z","shell.execute_reply":"2024-03-30T22:34:41.547883Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch: 0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m Woh_hh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hid_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWxh_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWxh_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWoh_hh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[22], line 71\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(inputs, prev_input, prev_hid_output, targets, Wxh_curr, Wxh_prev, Woh_hh, lr)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs): \u001b[38;5;66;03m# enumerate will return index value and value itself\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     dWxh_curr, dWxh_prev, dWoh_hh, loss, total_loss, total_words \u001b[38;5;241m=\u001b[39m \u001b[43mbptt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hid_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWxh_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWxh_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWoh_hh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     Wxh_curr \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m dWxh_curr\n\u001b[1;32m     73\u001b[0m     Wxh_prev \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m dWxh_prev\n","Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mbptt\u001b[0;34m(sent, prev_inputs, prev_hid_output, target, Wxh_curr, Wxh_prev, Woh_hh, total_loss, total_words)\u001b[0m\n\u001b[1;32m     37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent:\n\u001b[0;32m---> 39\u001b[0m     prev_input, h_t, output \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hid_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWxh_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWxh_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWoh_hh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     prev_hid_output \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     41\u001b[0m     prev_inputs \u001b[38;5;241m=\u001b[39m prev_input\n","Cell \u001b[0;32mIn[22], line 29\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(word, prev_inputs, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(word, prev_inputs, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh): \u001b[38;5;66;03m#Woh_hh = 1 initially\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     for word in sent:\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     h_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Wxh_curr, word) \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWxh_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_inputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Woh_hh, prev_hid_output)\n\u001b[1;32m     30\u001b[0m     output \u001b[38;5;241m=\u001b[39m sigmoid(h_t)\n\u001b[1;32m     31\u001b[0m     prev_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(word, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"binary_cross_entropy(-1,0)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T23:09:50.877020Z","iopub.execute_input":"2024-03-30T23:09:50.877694Z","iopub.status.idle":"2024-03-30T23:09:50.884499Z","shell.execute_reply.started":"2024-03-30T23:09:50.877660Z","shell.execute_reply":"2024-03-30T23:09:50.883508Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"9.992007221626415e-16"},"metadata":{}}]},{"cell_type":"code","source":"inputs = []\ntargets = []\nfor sentence in train_data:\n    tokens = sentence['tokens']\n    chunk_tags = sentence['chunk_tags']\n    pos_tags = sentence['pos_tags']\n    inputs.append(pos_tags)\n    targets.append(chunk_tags)\n#     print(pos_tags)\n# Calculate the index to split the list\nsplit_index = int(0.8 * len(inputs))\n\n# Split the list\ntrain_set = inputs[:split_index]\ntest_set = inputs[split_index:]\ntrain_target = targets[:split_index]\ntest_target = targets[split_index:]\n# targets = test_target","metadata":{"execution":{"iopub.status.busy":"2024-03-31T03:05:35.806113Z","iopub.execute_input":"2024-03-31T03:05:35.806476Z","iopub.status.idle":"2024-03-31T03:05:35.821601Z","shell.execute_reply.started":"2024-03-31T03:05:35.806447Z","shell.execute_reply":"2024-03-31T03:05:35.820618Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# pos_tags = [[1,4,3,1,4,4,3,1,4],[4,1,4,1,4,1,4,1,4]]\n# pos_tags = [[4,1,4,4,4,4,1,4,2,1,4,1,3,1,4,3,1,1,1,4,4,4,3,1,4,4,4,4,4,4,2,3,1,4,3,1,4], [2,1,1,4,4,1,4,4,4,4,4,2,1,4,4,4,4,2,4,4,4,2,1,4,4,2,1,4,1,1,1,4,3,4,4]]\n# one_hot(pos_tags)[0] # 4x1\nprev_input = np.array([[1,0,0,0,0]]).T #shape 5x1\nprev_hid_output = np.array([[0]]).T #shape 1x1 \n# targets = [[1,1,1,0,1,1,1,0,1],[1,0,0,0,1,1,1,0,1]]\n# targets = [[1,1,1,1,1,1,0,1,1,0,1,0,0,0,1,1,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,0,0,1,1,0,1], [1,0,0,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,0,1]]\nWxh_curr = np.random.randn(1,4)\nWxh_prev = np.random.rand(1,5)\nWoh_hh = np.random.randn(1,1)\nprev_inp = []\nh_ts = []\noutputs = []\ntotal_loss=0\ntotal_words=0\nlr = 0.0001\nmomentum = 0.1\nlosses = []\nepochs = []\n\nfor epoch in range(2):\n    print(\"epoch:\", epoch)\n    epochs.append(epoch)\n    total_loss=0\n    total_words=0\n    for idx, sent in enumerate(inputs):\n        prev_input = np.array([[1,0,0,0,0]]).T\n        prev_hid_output = np.array([[0]]).T \n        for word in one_hot(sent):\n#             print(prev_input)\n            prev_inputs, h_t, output = forward(word, prev_input, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh)\n            prev_hid_output = output\n            prev_input = prev_inputs\n            prev_inp.append(prev_inputs)#this is an array containing output of all the words\n            h_ts.append(h_t)\n            outputs.append(output)\n#         print(prev_inp)\n\n        dWxh_curr, dWxh_prev, dWoh_hh = np.zeros_like(Wxh_curr), np.zeros_like(Wxh_prev), np.zeros_like(Woh_hh)\n        dh_next = np.zeros_like(prev_hid_output)\n        # Initialize momentum terms\n        mWxh_curr, mWxh_prev, mWoh_hh = np.zeros_like(Wxh_curr), np.zeros_like(Wxh_prev), np.zeros_like(Woh_hh)\n        \n        for i in reversed(range(len(one_hot(sent)))):\n            loss = binary_cross_entropy(outputs[i-1], targets[idx][i])\n#             print(\"loss:\", loss)\n            total_loss += loss\n#             print(\"total_loss:\", total_loss)\n            total_words += 1\n#             print(\"total_words:\", total_words)\n            dWoh_hh += np.dot(bce_derivative(outputs[i], targets[idx][i]), sigmoid_derivative(h_ts[i]) * outputs[i].T) \n            dh = np.dot(sigmoid_derivative(h_ts[i]), bce_derivative(outputs[i], targets[idx][i])) + dh_next\n            dWxh_curr += np.dot(dh, one_hot(sent)[i].T)\n            dWxh_prev += np.dot(dh, prev_inp[i].T)\n            dh_next = np.dot(Woh_hh.T, dh)\n#          print(\"Wxh_cur_init:\", Wxh_curr)\n#          Wxh_curr -= lr * dWxh_curr\n#          print(\"Wxh_cur:\", Wxh_curr)\n#          Wxh_prev -= lr * dWxh_prev\n#          print(\"Woh_hh_init:\", Woh_hh)\n#          Woh_hh -= lr * dWoh_hh\n        # Update momentum terms\n        mWxh_curr = momentum * mWxh_curr + lr * dWxh_curr\n        mWxh_prev = momentum * mWxh_prev + lr * dWxh_prev\n        mWoh_hh = momentum * mWoh_hh + lr * dWoh_hh\n\n        # Update weights with momentum\n        Wxh_curr -= mWxh_curr\n        Wxh_prev -= mWxh_prev\n        Woh_hh -= mWoh_hh\n#         print(\"weights_updated\")\n#         print(\"end\")\n#     print(\"Woh_hh:\", Woh_hh)\n    average_loss = total_loss/total_words\n    print(\"total_loss:\", total_loss)\n    print(\"total_words:\", total_words)\n    print(\"average_loss:\", average_loss)\n    losses.append(average_loss)\n    print(\"loss:\", loss)\n\nimport matplotlib.pyplot as plt\n\n# Data\n# x = [1, 2, 3, 4, 5, 6]\n# y = [65, 95, 46, 32, 61, 72]\n\n# Plot\nplt.plot(epochs, losses, marker='o', linestyle='-')\n\n# Add labels and title\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Plot of X vs Y')\n\n# Display the plot\nplt.grid(True)\nplt.show()\n# print(\"dWoh_hh:\", dWoh_hh)\n# print(\"dh:\", dh)\n# print(\"dWxh_curr:\", dWxh_curr)\n# print(\"dWxh_prev:\", dWxh_prev)\n# print(\"dh_next:\", dh_next)\n# print(\"prev_inputs:\", prev_inp[0].shape)\n# print(\"h_ts:\", h_ts[0].shape)\n# print(\"outputs:\", outputs)\n# word = [1,0,0,0]\n# prev_inputs = np.insert(word, 0, 0)\n# print(prev_inputs)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T03:32:51.300661Z","iopub.execute_input":"2024-03-31T03:32:51.301037Z","iopub.status.idle":"2024-03-31T03:34:31.513423Z","shell.execute_reply.started":"2024-03-31T03:32:51.301007Z","shell.execute_reply":"2024-03-31T03:34:31.512557Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"epoch: 0\ntotal_loss: 225192.12914603236\ntotal_words: 203621\naverage_loss: 1.1059376446733509\nloss: 34.538776394910684\nepoch: 1\ntotal_loss: 545670.5836071519\ntotal_words: 203621\naverage_loss: 2.6798345141569477\nloss: 9.992007221626415e-16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"Wxh_curr","metadata":{"execution":{"iopub.status.busy":"2024-03-31T03:29:09.685484Z","iopub.execute_input":"2024-03-31T03:29:09.686210Z","iopub.status.idle":"2024-03-31T03:29:09.692101Z","shell.execute_reply.started":"2024-03-31T03:29:09.686179Z","shell.execute_reply":"2024-03-31T03:29:09.691139Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([[1.15781493e+124, 3.43127223e+111, 3.07969661e+119,\n        5.76239619e+121]])"},"metadata":{}}]},{"cell_type":"code","source":"test_data = load_data('/kaggle/input/conll2003-posdataset/test.jsonl')\n\ntest_inputs = []\ntest_targets = []\nfor sentence in test_data:\n    tokens = sentence['tokens']\n    chunk_tags = sentence['chunk_tags']\n    pos_tags = sentence['pos_tags']\n    test_inputs.append(pos_tags)\n    test_targets.append(chunk_tags)\n\ndef compute_accuracy(predicted, ground_truth):\n    \"\"\"\n    Computes accuracy given predicted array and ground truth (train) array.\n\n    Args:\n    predicted (list or numpy array): Array containing predicted labels.\n    ground_truth (list or numpy array): Array containing ground truth labels.\n\n    Returns:\n    float: Accuracy of the predictions.\n    \"\"\"\n    if len(predicted) != len(ground_truth):\n        raise ValueError(\"The length of predicted and ground_truth arrays must be the same.\")\n\n    correct = sum(1 for pred, truth in zip(predicted, ground_truth) if pred == truth)\n    total = len(ground_truth)\n    accuracy = correct / total\n\n    return accuracy\n\nfin_outs=[]\n# split_index = int(0.1 * len(train_set))\n# train_set_n=train_set[:split_index]\n# train_target_n=train_target[:split_index]\nfor test,chunk in zip(test_inputs,test_targets):\n#     test = [1,1,1,1,4,4,4,4,4,4]\n#     chunk = [1,0,0,0,1,1,1,0,0,1]\n    outputs = []\n    for word in one_hot(test):\n        prev_inputs, h_t, output = forward(word, prev_input, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh)\n        outputs.append(output)\n    fin_outs.append(outputs)\n\npredicted_labels=fin_outs\nground_truth_labels=test_targets\naccuracy = compute_accuracy(predicted_labels, ground_truth_labels)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T03:30:21.313770Z","iopub.execute_input":"2024-03-31T03:30:21.314600Z","iopub.status.idle":"2024-03-31T03:30:23.578893Z","shell.execute_reply.started":"2024-03-31T03:30:21.314567Z","shell.execute_reply":"2024-03-31T03:30:23.578032Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Accuracy: 0.06573993628728642\n","output_type":"stream"}]},{"cell_type":"code","source":"test = [1,1,1,1,4,4,4,4,4,4]\nchunk = [1,0,0,0,1,1,1,0,0,1]\noutputs = []\nfor word in one_hot(test):\n    prev_inputs, h_t, output = forward(word, prev_input, prev_hid_output, Wxh_curr, Wxh_prev, Woh_hh)\n    outputs.append(output)\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T03:04:36.463391Z","iopub.execute_input":"2024-03-31T03:04:36.463717Z","iopub.status.idle":"2024-03-31T03:04:36.474073Z","shell.execute_reply.started":"2024-03-31T03:04:36.463692Z","shell.execute_reply":"2024-03-31T03:04:36.472929Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[array([[2.80125635e-06]]), array([[2.80125635e-06]]), array([[2.80125635e-06]]), array([[2.80125635e-06]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]]), array([[1.]])]\n","output_type":"stream"}]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-03-30T23:22:12.917616Z","iopub.execute_input":"2024-03-30T23:22:12.917948Z","iopub.status.idle":"2024-03-30T23:22:12.923970Z","shell.execute_reply.started":"2024-03-30T23:22:12.917925Z","shell.execute_reply":"2024-03-30T23:22:12.922996Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"array([[0.83170174]])"},"metadata":{}}]},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef forward(input, prev_input, h_t_1, W_inp, W_prev, W_ht):\n    input = input.reshape((-1, 1))\n    prev_input = prev_input.reshape((-1, 1))\n    h_t = np.dot(W_inp, input) + np.dot(W_prev, prev_input) + np.dot(W_ht, h_t_1)\n    pred_output = sigmoid(h_t)\n    return pred_output, h_t\ndef bptt(y_pred, input, y, h_t, prev_input, W_inp, W_prev, W_ht, lr):\n  # error = y - y_pred\n    dLdW_inp = 0\n    dLdW_prev = 0\n    dLdW_ht = 0\n    for i in range(len(input)):\n        dLdW_inp += np.dot((y[i] - y_pred[i])*y_pred[i]*(1-y_pred[i]),one_hot(input[i].T))\n        dLdW_prev += np.dot((y[i] - y_pred[i])*y_pred[i]*(1-y_pred[i]),prev_input[i].T)\n        dLdW_ht += np.dot((y[i] - y_pred[i])*y_pred[i]*(1-y_pred[i]),h_t[i].T)\n        W_inp = W_inp - lr*dLdW_inp\n        W_prev = W_prev - lr*dLdW_prev\n        W_ht = W_ht - lr*dLdW_ht\n    return W_inp, W_prev, W_ht\ndef one_hot(pos):\n    one_hot = np.array([[0,0,0,0]])\n    one_hot[0][pos-1] = 1\n    return np.array(one_hot)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_tags = [[1, 4, 3, 1, 4, 4, 3, 1, 4],[4,1,4,1,4,1,4,1,4],[1,4,4,1,4,1,3,1,4,4,1,4,3,1,4]]\nchunk_tags = [[1,1,1,0,1,1,1,0,1], [1,0,0,0,1,1,1,0,1],[1,1,1,0,1,1,1,0,1,1,0,1,1,0,1]]\n\nW_inp  = np.random.randn(1,4) # (4 x )\nprint(W_inp.shape)\nW_prev = np.random.randn(1,5) # (5 x )\nW_ht = np.array([[1]]) #(1)\nh_t_1 = np.array([[0]])\nlr = 0.01\n# thr = 1.0\n\n# prev_input = np.zeros((5, 1))\npred_output_list = []\nh_t_list = []\nprev_input_list = []\nfor i in range(len(pos_tags)):\n    # output = chunk_tags[i]\n    # prev_input = [1,0,0,0,0]\n    prev_input = np.zeros((5, 1))\n    # prev_input = np.array([prev_input])\n    for j in range(len(pos_tags[i])):\n        input = one_hot(pos_tags[i][j]).reshape((-1,1))\n        pred_out, h_t = forward(input, prev_input, h_t_1, W_inp, W_prev, W_ht)\n        pred_output_list.append(pred_out)\n        h_t_list.append(h_t)\n        h_t_1 = h_t\n        prev_input_list.append(prev_input)\n        prev_input = np.insert(input, 0, 0, axis=0)\n        # prev_input = np.vstack((np.expand_dims(input, axis=1), prev_input[1:]))\n        # prev_input = np.vstack((np.expand_dims(input, axis=1), prev_input[1:]))\n        # W_inp, W_prev, W_ht = bptt(pred_output_list, pos_tags[i], chunk_tags[i], h_t_list, prev_input_list, W_inp, W_prev, W_ht, lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T22:40:46.921657Z","iopub.execute_input":"2024-03-30T22:40:46.922375Z","iopub.status.idle":"2024-03-30T22:40:46.976195Z","shell.execute_reply.started":"2024-03-30T22:40:46.922343Z","shell.execute_reply":"2024-03-30T22:40:46.975076Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"(1, 4)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# prev_input = np.array([prev_input])\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pos_tags[i])):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_tags\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m     pred_out, h_t \u001b[38;5;241m=\u001b[39m forward(\u001b[38;5;28minput\u001b[39m, prev_input, h_t_1, W_inp, W_prev, W_ht)\n\u001b[1;32m     24\u001b[0m     pred_output_list\u001b[38;5;241m.\u001b[39mappend(pred_out)\n","Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(pos_tags)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot\u001b[39m(pos_tags): \u001b[38;5;66;03m#returns an array of one hot encoding of each words in a sentence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpos_tags\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m4\u001b[39m))  \u001b[38;5;66;03m# 4 is the number of POS tag categories\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pos_tags):\n\u001b[1;32m      4\u001b[0m         X_new[\u001b[38;5;241m0\u001b[39m, i, tag\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"],"ename":"TypeError","evalue":"object of type 'int' has no len()","output_type":"error"}]},{"cell_type":"code","source":"# Testing the model on test data\nfor sentence in test_data:\n    tokens = sentence['tokens']\n    pos_tags = sentence['pos_tags']\n    print(sentence)\n    \n    # Predict chunk tags for each word in the sentence\n    predicted_chunk_tags = []\n    for i in range(len(tokens)):\n        output, _ = forward_propagation([pos_tags[i]])\n        print(output)\n#         break\n        prediction = output[0][0]\n        predicted_chunk_tags.append(1 if prediction > 0.5 else 0)\n    \n    print(\"Predicted chunk tags:\", predicted_chunk_tags)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.216045Z","iopub.status.idle":"2024-03-30T20:45:50.216378Z","shell.execute_reply.started":"2024-03-30T20:45:50.216215Z","shell.execute_reply":"2024-03-30T20:45:50.216229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# Sample data for testing (new POS tag sequence)\nnew_pos_tags = [1, 4, 3, 1, 4, 4, 3, 1, 4]\nX_new = np.zeros((1, len(new_pos_tags), 4))  # 4 is the number of POS tag categories\nfor i, tag in enumerate(new_pos_tags):\n    X_new[0, i, tag-1] = 1","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.217494Z","iopub.status.idle":"2024-03-30T20:45:50.217836Z","shell.execute_reply.started":"2024-03-30T20:45:50.217671Z","shell.execute_reply":"2024-03-30T20:45:50.217685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_new","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.219148Z","iopub.status.idle":"2024-03-30T20:45:50.219470Z","shell.execute_reply.started":"2024-03-30T20:45:50.219312Z","shell.execute_reply":"2024-03-30T20:45:50.219325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \ndef binary_cross_entropy(y_pred, y_actual):\n    epsilon = 1e-15  # small value to avoid division by zero\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # clip values to avoid NaNs in log\n    loss = -(y_actual * np.log(y_pred) + (1 - y_actual) * np.log(1 - y_pred))\n    return np.mean(loss)\n\n# Example data\ny_pred = np.array([1, 0, 0, 1, 1, 0])\ny_actual = np.array([1, 1, 0, 0, 1, 0])\n\n# Calculate loss\nlosses = binary_cross_entropy(y_pred, y_actual)\naverage_loss = np.mean(losses)\n\nprint(\"Losses for each element:\", losses)\nprint(\"Average loss:\", average_loss)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.220587Z","iopub.status.idle":"2024-03-30T20:45:50.220908Z","shell.execute_reply.started":"2024-03-30T20:45:50.220750Z","shell.execute_reply":"2024-03-30T20:45:50.220763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \n# Sample data for testing (new POS tag sequence)\n# new_pos_tags = [1, 4, 3, 1, 4, 4, 3, 1, 4]\ndef one_hot(pos_tags):\n    X_new = np.zeros((1, len(pos_tags), 4))  # 4 is the number of POS tag categories\n    for i, tag in enumerate(pos_tags):\n        X_new[0, i, tag-1] = 1\n    # print(X_new)\n    arrays = [np.reshape(array, (4, 1)) for array in X_new[0]] #One hot encoded arrays\n    return arrays\n\n# chunk_tags = [1,1,1,0,1,1,1,0,1]\nprev_input = [1,0,0,0,0] #it should be 5x1, I will take its transpose later\nprev_input = np.array([prev_input])\n# print(prev_input)\nW_input = np.random.randn(1,4)\nW_prev_input = np.random.rand(1,5)\nW_output = np.array([[1]])\noutput = np.array([[0]])\nthreshold = 1.00\nlr = 0.01\nnum_epochs = 20\nnew_pos_tags = [[1, 4, 3, 1, 4, 4, 3, 1, 4],[4,1,4,1,4,1,4,1,4]]\nchunk_tags = [[1,1,1,0,1,1,1,0,1], [1,0,0,0,1,1,1,0,1]]\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef forward(W_input, W_prev_input, prev_input, W_output,\n            output,  threshold, arrays, output_list):\n#     print(\"W_prev_inpt_shp:\", W_prev_input.shape)\n#     print(\"prev_input_shp:\",prev_input.shape)\n#     print(\"W_input_shp:\", W_input.shape)\n#     print(\"W_output_shp:\", W_output.shape)\n#     print(\"Output_shape:\", output.shape)\n    for i in arrays:\n        h_t = np.dot(W_input, i) + np.dot(W_prev_input,prev_input.T) + np.dot(W_output,output)\n#         print(\"h_t_shape:\",h_t)\n        output = sigmoid(h_t) #np.array([[1]]) if h_t > threshold else np.array([[0]])\n        print(\"hello_output:\",output.shape)\n        output_list.append(output)\n        prev_input = np.insert(i, 0, 0, axis=0)\n        prev_input = np.array([prev_input])\n        \n    return output_list\n    \n# def bptt():\n#     dL_W_input = \n# def backward(num_epochs, W_input, W_prev_input, prev_input, W_output,\n#              arrays, threshold, new_pos_tags, chunk_tags):\nfor i in range(num_epochs):\n    print(\"Epoch:\", i)\n    for j in range(2):\n        k = 0\n        output_list = []\n    #     print(\"W_prev_inpt:\", W_prev_input.shape)\n    #     print(\"prev_input:\",prev_input)\n        \n        prediction = forward(W_input, W_prev_input, prev_input, W_output,\n            output, threshold, one_hot(new_pos_tags[j]), output_list)\n\n    #     loss = binary_cross_entropy_loss(prediction, chunk_tags)\n    #     print(prediction)\n    #     print(np.array(chunk_tags))\n    #     print(np.array(prediction).T)\n        loss = np.array(chunk_tags[j]) - np.array(prediction).T\n        loss = np.reshape(loss, (len(chunk_tags[j]),))\n        total_loss = np.mean(loss)\n#         print(\"total_loss:\", total_loss)\n#         total_loss = binary_cross_entropy(np.array(prediction).T, np.array(chunk_tags[j]))\n    #     print(loss)\n    #     print(total_loss)\n    #     print(W_input.shape)\n    #     print(arrays[j])\n    #     print(arrays[j].shape)\n#         print(one_hot(new_pos_tags[j])[k])\n        \n        for p in range(len(one_hot(new_pos_tags[j]))):\n            \n        \n        # Correct the code of BPTT\n        W_input = W_input + total_loss * lr * one_hot(new_pos_tags[j])[k].T #problem is here, this line not running for all the words in the sentence\n    #     print(W_input.shape)\n    #     print(prev_input.shape)\n    #     print(\"total_loss_shape:\",total_loss.shape)\n        W_prev_input = W_prev_input + total_loss * lr * prev_input \n#         k = k+1\n        print(\"output_list:\",np.array(output_list).T)\n#     W_output = W_output + loss * lr * \n#     output_list.empty()\n#     total_loss += loss\n#     total_words += 1\n\n# reshaped_output_array = np.reshape(np.array(output_list), (len(new_pos_tags),))\n# for i in \n# print(np.dot(W_prev_input,prev_input.T))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.222262Z","iopub.status.idle":"2024-03-30T20:45:50.222568Z","shell.execute_reply.started":"2024-03-30T20:45:50.222410Z","shell.execute_reply":"2024-03-30T20:45:50.222422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# One-hot encoding for POS tags: [OT:1, NN:2, JJ: 3, DT:4]\n# X_train = [\n#     {\"pos_tags\": [1, 4, 3, 1, 4, 4, 3, 1, 4]},\n#     {\"pos_tags\": [4,1,4,1,4,1,4,1,4]}\n# ]\n\n# y_train = [\n#     {\"chunk_tags\": [1,1,1,0,1,1,1,0,1]},\n#     {\"chunk_tags\": [1,0,0,0,1,1,1,0,1]}\n# ]\nX = [1, 4, 3, 1, 4, 4, 3, 1, 4]\ny = [1,1,1,0,1,1,1,0,1]\noutput_list = [0, 1, 1, 0, 1, 1, 1, 0, 1]\nW_in = np.random.randn(1,1)\nW_rec = np.random.rand(1,1)\ndef backward(X, Y, outputs):\n    T = len(X)\n    dW_in = np.zeros_like((1, 1))\n    dW_rec = np.zeros_like((1, 1))\n    delta_next = np.zeros((1, 1))\n\n    for t in reversed(range(T)):\n        error = outputs[:, t].reshape(-1,1) - Y[t]\n        delta_out = error\n        dW_rec += np.dot(delta_out, self.hidden_states[:, t].reshape(1,-1))\n        delta_hidden = np.dot(self.W_rec.T, delta_next)\n        delta = delta_hidden * self.hidden_states[:, t+1] * (1 - self.hidden_states[:, t+1])\n        dW_in += np.dot(delta, X[t].reshape(1,-1))\n        delta_next = delta\n\n    # Update weights and biases\n    W_in -= self.learning_rate * dW_in\n    W_rec -= self.learning_rate * dW_rec\n    \nres = backward(X, y, output_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.223565Z","iopub.status.idle":"2024-03-30T20:45:50.223885Z","shell.execute_reply.started":"2024-03-30T20:45:50.223726Z","shell.execute_reply":"2024-03-30T20:45:50.223739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bptt_train(lr, X_train_data,Y_train_data, num_epochs, W_input, W_prev_input, prev_input, W_output,\n             arrays, threshold, new_pos_tags, chunk_tags,train_size):\n    for i in range(num_epochs):\n        for new_pos_tags in X_train_data:\n            X_new = np.zeros((1, len(new_pos_tags), 4))  # 4 is the number of POS tag categories\n            for i, tag in enumerate(new_pos_tags):\n                X_new[0, i, tag-1] = 1\n            # print(X_new)\n            arrays = [np.reshape(array, (4, 1)) for array in X_new[0]] #One hot encoded arrays\n            backward(W_input, W_prev_input, prev_input, W_output,\n             arrays, threshold, new_pos_tags, chunk_tags,train_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.224890Z","iopub.status.idle":"2024-03-30T20:45:50.225224Z","shell.execute_reply.started":"2024-03-30T20:45:50.225065Z","shell.execute_reply":"2024-03-30T20:45:50.225078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample data for testing (new POS tag sequence)\nnew_pos_tags = [1, 4, 3, 1, 4, 4, 3, 1, 4]\nX_new = np.zeros((1, len(new_pos_tags), 4))  # 4 is the number of POS tag categories\nfor i, tag in enumerate(new_pos_tags):\n    X_new[0, i, tag-1] = 1\n# print(X_new)\narrays = [np.reshape(array, (4, 1)) for array in X_new[0]] #One hot encoded arrays\nchunk_tags = [1,1,1,0,1,1,1,0,1]\nprev_input = [1,0,0,0,0] #it should be 5x1, I will take its transpose later\nprev_input = np.array([prev_input])\n# print(prev_input)\nW_input = np.random.randn(1,4)\nW_prev_input = np.random.rand(1,5)\nW_output = np.array([[1]])\noutput = np.array([[0]])\nthreshold = 0.5\nlr = 0.1\n\nbptt_train(lr, W_input, W_prev_input, prev_input, W_output,\n             arrays, threshold, new_pos_tags, chunk_tags,train_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.226299Z","iopub.status.idle":"2024-03-30T20:45:50.226641Z","shell.execute_reply.started":"2024-03-30T20:45:50.226483Z","shell.execute_reply":"2024-03-30T20:45:50.226496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass SingleRecurrentPerceptron:\n    def __init__(self, input_size, learning_rate=0.1):\n        self.input_size = input_size\n        self.learning_rate = learning_rate\n        self.weights = np.zeros(input_size)\n        self.threshold = 0\n\n    def activation_function(self, x):\n        return 1 if x >= self.threshold else 0\n    \n    def forward(self, inputs):\n        output = np.dot(inputs, self.weights)\n        return self.activation_function(output)\n    \n    def train(self, X_train, y_train, epochs=1):\n        for _ in range(epochs):\n            print(\"Epoch:\", _)\n            for inputs, target in zip(X_train, y_train):\n                predicted = self.forward(inputs)\n                error = target - predicted\n                self.weights += self.learning_rate * error * inputs\n\n    \n\n# Example usage:\n# One-hot encoding for POS tags: [OT:1, NN:2, JJ: 3, DT:4]\nX_train = np.array([[0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1, 0, 0, 0], \n                    [1, 0, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0]])\ny_train = np.array([1, 0, 0, 0, 1, 1, 1, 0])\n\nmodel = SingleRecurrentPerceptron(input_size=4)\n\n# Training for one epoch as described in the example\nmodel.train(X_train, y_train)\n\n# Predicting the output for each step as described in the example\noutputs = []\nX_test = np.array([[0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1, 0, 0, 0], \n                    [1, 0, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0]])\nfor inputs in X_test:\n    prediction = model.forward(inputs)\n    outputs.append(prediction)\n\nprint(\"Final Noun Phrase Chunks:\", outputs)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.227611Z","iopub.status.idle":"2024-03-30T20:45:50.227927Z","shell.execute_reply.started":"2024-03-30T20:45:50.227769Z","shell.execute_reply":"2024-03-30T20:45:50.227782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nclass SingleRecurrentPerceptron:\n    def __init__(self, input_size, learning_rate=0.1):\n        self.input_size = input_size\n        self.learning_rate = learning_rate\n        self.V_weights = np.random.randn(input_size)\n        self.W_weights = np.random.randn(input_size)\n        print(\"weights:\",self.W_weights)\n        self.threshold = 0.75\n\n#     def activation_function(self, x):\n#         return 1 if x >= self.threshold else 0\n    def activation_function(self, x):\n        return np.where(x >= self.threshold, 1, 0)\n\n\n    def train(self, X_train, y_train, epochs=10):\n        for _ in range(epochs):\n            print(\"Epoch:\", _)\n            for inputs, target in zip(X_train, y_train):\n#                 print(\"inputs_shape:\", inputs.shape)\n                predicted = self.predict(inputs)\n                # Adjust lengths of target and predicted arrays if necessary\n                if len(target) != len(predicted):\n                    target = target[:len(predicted)]  # Trim target if it's longer than predicted\n#                 error = target - predicted\n                error = log_likelihood_loss(target, predicted)\n                print(\"Error:\", error)\n                error_expanded = np.expand_dims(error, axis=1)  # Expand error to match the shape of inputs\n#                 print(\"error_shape:\",error.shape)\n#                 print(\"W_weights_shape:\",self.W_weights.shape)\n                self.W_weights += self.learning_rate * np.sum(error_expanded * inputs, axis=0)\n#                 print((self.learning_rate * error * inputs).shape)\n#                 print((self.learning_rate * np.sum(error_expanded * inputs, axis=0)).shape)\n#                 self.W_weights += self.learning_rate * error * inputs\n                \n#                 print(self.W_weights.shape)\n                self.V_weights += self.learning_rate * np.sum(error_expanded * inputs, axis=0)\n#                 self.V_weights += self.learning_rate * error * inputs\n\n    def predict(self, inputs):\n        net = np.dot(inputs, self.W_weights) + np.dot(inputs, self.V_weights)\n        output = self.activation_function(net)\n        return output\n\n# Function to convert POS tags into one-hot encoding\ndef pos_to_one_hot(pos_tags):\n    tag_mapping = {\"OT\": 1, \"NN\": 2, \"JJ\": 3, \"DT\": 4}\n    one_hot = np.zeros((len(pos_tags), len(tag_mapping)))\n    for i, tag in enumerate(pos_tags):\n        one_hot[i][tag_mapping[tag] - 1] = 1\n    return one_hot\n\n# Example usage:\n# Example X_train and y_train datasets\nX_train = [\n    {\"pos_tags\": [\"OT\", \"DT\", \"JJ\", \"NN\", \"OT\", \"DT\", \"JJ\", \"NN\", \"OT\"]},\n    {\"pos_tags\": [\"NN\", \"OT\", \"DT\", \"JJ\", \"NN\", \"OT\", \"DT\", \"JJ\", \"NN\"]},\n    {\"pos_tags\": [\"DT\", \"JJ\", \"NN\", \"OT\", \"DT\", \"JJ\", \"NN\", \"OT\", \"DT\"]}\n]\n\ny_train = [\n    {\"chunk_tags\": [1, 1, 1, 0, 1, 1, 1, 0, 1]},\n    {\"chunk_tags\": [1, 0, 0, 0, 1, 1, 1, 0, 1]},\n    {\"chunk_tags\": [1, 0, 0, 0, 1, 1, 1, 0, 0, 1]}\n]\n\n# Convert POS tags to one-hot encoding\nX_train_one_hot = [pos_to_one_hot(example[\"pos_tags\"]) for example in X_train]\n\n# Initialize and train the model\nmodel = SingleRecurrentPerceptron(input_size=len(X_train_one_hot[0][0]))\nmodel.train(X_train_one_hot, [np.array(example[\"chunk_tags\"]) for example in y_train])\n\n\nX_test= [\n    {\"pos_tags\":[\"OT\",\"OT\",\"OT\",\"OT\",\"DT\",\"DT\",\"JJ\",\"OT\",\"DT\",\"OT\",\"DT\"]},\n    {\"pos_tags\":[\"JJ\",\"OT\",\"DT\",\"DT\",\"OT\",\"DT\",\"NN\",\"OT\",\"DT\"]},\n    {\"pos_tags\":[\"JJ\",\"DT\",\"OT\",\"DT\",\"DT\",\"OT\",\"DT\",\"DT\",\"DT\",\"OT\",\"DT\"]}\n]\n\ny_test = [\n    {\"chunk_tags\":[1,0,0,0,1,1,0,0,1,1,1]},\n    {\"chunk_tags\":[1,0,1,1,0,1,1,0,1]},\n    {\"chunk_tags\":[1,1,1,1,1,0,1,1,1,0,1]}\n]\n\n# Convert POS tags to one-hot encoding\nX_test_one_hot = [pos_to_one_hot(example[\"pos_tags\"]) for example in X_test]\n\n# Predicting the output for each step as described in the example\nfor i, inputs in enumerate(X_test_one_hot):\n    prediction = model.predict(inputs)\n    print(\"Prediction for example\", i+1, \":\", prediction)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.228866Z","iopub.status.idle":"2024-03-30T20:45:50.229202Z","shell.execute_reply.started":"2024-03-30T20:45:50.229041Z","shell.execute_reply":"2024-03-30T20:45:50.229057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef log_likelihood_loss(y_true, y_pred):\n    epsilon = 1e-15  # Small value to prevent log(0)\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid log(0) or log(1)\n    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\n# Example usage\ny_true = np.array([1, 0, 1, 1, 0, 1, 1, 0, 1])\ny_pred = np.array([0, 0, 0, 0, 0, 1, 1, 0, 0])\nloss = log_likelihood_loss(y_true, y_pred)\nprint(\"Log likelihood loss:\", loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.230671Z","iopub.status.idle":"2024-03-30T20:45:50.230972Z","shell.execute_reply.started":"2024-03-30T20:45:50.230821Z","shell.execute_reply":"2024-03-30T20:45:50.230834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef log_likelihood_loss(y_true, y_pred):\n    epsilon = 1e-15  # Small value to prevent log(0)\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid log(0) or log(1)\n    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    return loss\n\n# Example usage\ny_true = np.array([1, 0, 1, 1, 0, 1, 1, 0, 1])\ny_pred = np.array([0, 0, 0, 0, 0, 1, 1, 0, 0])\nloss = log_likelihood_loss(y_true, y_pred)\nprint(\"Log likelihood loss:\", loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T20:45:50.232634Z","iopub.status.idle":"2024-03-30T20:45:50.232958Z","shell.execute_reply.started":"2024-03-30T20:45:50.232799Z","shell.execute_reply":"2024-03-30T20:45:50.232813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}